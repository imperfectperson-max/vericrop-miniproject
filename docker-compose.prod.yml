version: '3.8'

#
# VeriCrop Production Docker Compose Configuration
# 
# IMPORTANT: Before running in production:
# 1. Copy .env.production.example to .env and fill in all values
# 2. Ensure vericrop_quality_model.onnx is present in docker/ml-service/model/
# 3. Set up proper secret management (e.g., Docker secrets, Vault, AWS Secrets Manager)
# 4. Configure backup strategy for postgres_data and ledger volumes
# 5. Set up monitoring and alerting for all services
# 6. Review and adjust resource limits based on your infrastructure
#

services:
  # PostgreSQL Database for Batch Metadata
  postgres:
    image: postgres:15
    container_name: vericrop-postgres-prod
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - vericrop-network
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Apache Kafka for Event Streaming
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: vericrop-zookeeper-prod
    restart: unless-stopped
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_logs:/var/lib/zookeeper/log
    networks:
      - vericrop-network
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | nc localhost 2181 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: vericrop-kafka-prod
    restart: unless-stopped
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "${KAFKA_PORT:-9092}:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://${KAFKA_HOST:-localhost}:${KAFKA_PORT:-9092}
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_RETENTION_BYTES: 1073741824
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - vericrop-network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ML Service (FastAPI) for Quality Predictions
  ml-service:
    build:
      context: ./docker/ml-service
      dockerfile: Dockerfile
    image: vericrop/ml-service:${VERSION:-latest}
    container_name: vericrop-ml-service-prod
    restart: unless-stopped
    ports:
      - "${ML_SERVICE_PORT:-8000}:8000"
    environment:
      - VERICROP_MODE=prod  # Production mode - requires ONNX model
      - VERICROP_LOAD_DEMO=false  # Demo mode disabled in production
      - PORT=8000
    volumes:
      # Mount model directory - ONNX model must be present for production
      - ./docker/ml-service/model:/app/model:ro
    networks:
      - vericrop-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Airflow Database
  postgres-airflow:
    image: postgres:13
    container_name: vericrop-postgres-airflow-prod
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${AIRFLOW_DB_USER}
      POSTGRES_PASSWORD: ${AIRFLOW_DB_PASSWORD}
      POSTGRES_DB: ${AIRFLOW_DB_NAME}
    volumes:
      - postgres_airflow_data:/var/lib/postgresql/data
    networks:
      - vericrop-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${AIRFLOW_DB_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Airflow Webserver
  airflow-webserver:
    image: apache/airflow:2.7.1
    container_name: vericrop-airflow-webserver-prod
    restart: unless-stopped
    depends_on:
      postgres-airflow:
        condition: service_healthy
      kafka:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@postgres-airflow:5432/${AIRFLOW_DB_NAME}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=false  # Security: Don't expose config in production
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS:-kafka:29092}
      - VERICROP_API_URL=${VERICROP_API_URL:-http://vericrop-gui:8080}
      - ML_SERVICE_URL=${ML_SERVICE_URL:-http://ml-service:8000}
    volumes:
      - ./airflow/dags:/opt/airflow/dags:ro  # Read-only in production
      - airflow_logs:/opt/airflow/logs
    ports:
      - "${AIRFLOW_WEBSERVER_PORT:-8082}:8080"
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '1.5'
          memory: 2G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - vericrop-network

  # Airflow Scheduler
  airflow-scheduler:
    image: apache/airflow:2.7.1
    container_name: vericrop-airflow-scheduler-prod
    restart: unless-stopped
    depends_on:
      postgres-airflow:
        condition: service_healthy
      kafka:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@postgres-airflow:5432/${AIRFLOW_DB_NAME}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS:-kafka:29092}
      - VERICROP_API_URL=${VERICROP_API_URL:-http://vericrop-gui:8080}
      - ML_SERVICE_URL=${ML_SERVICE_URL:-http://ml-service:8000}
    volumes:
      - ./airflow/dags:/opt/airflow/dags:ro
      - airflow_logs:/opt/airflow/logs
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $(hostname)"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '1.5'
          memory: 2G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - vericrop-network

volumes:
  postgres_data:
    driver: local
  postgres_airflow_data:
    driver: local
  zookeeper_data:
    driver: local
  zookeeper_logs:
    driver: local
  kafka_data:
    driver: local
  airflow_logs:
    driver: local

networks:
  vericrop-network:
    driver: bridge

# Production Deployment Notes:
# 
# 1. Secrets Management:
#    - Use Docker secrets or external secret managers
#    - Never commit .env files with real credentials
#    - Rotate secrets regularly
# 
# 2. Monitoring:
#    - Set up Prometheus to scrape /actuator/prometheus endpoints
#    - Configure alerting for service health failures
#    - Monitor resource usage and scale as needed
# 
# 3. Backups:
#    - Regular automated backups of postgres_data volume
#    - Backup ledger files from persistent storage
#    - Test restore procedures regularly
# 
# 4. Security:
#    - Use TLS/SSL for external communications
#    - Implement network policies to restrict inter-service communication
#    - Regular security updates for base images
#    - Implement authentication/authorization for Airflow
# 
# 5. High Availability:
#    - Consider multi-replica Kafka setup
#    - Use managed PostgreSQL service for better reliability
#    - Implement load balancing for ML service
