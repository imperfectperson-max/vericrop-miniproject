# Airflow Configuration
# Copy this file to .env and customize as needed

# ============================================================================
# Core Configuration
# ============================================================================
AIRFLOW_HOME=/opt/airflow
AIRFLOW_VERSION=2.7.1
PYTHON_VERSION=3.11

# ============================================================================
# Database Configuration (Metadata Store)
# ============================================================================
# PostgreSQL (recommended for production)
AIRFLOW_DB_TYPE=postgresql
AIRFLOW_DB_HOST=postgres
AIRFLOW_DB_PORT=5432
AIRFLOW_DB_NAME=airflow
AIRFLOW_DB_USER=airflow
AIRFLOW_DB_PASSWORD=<CHANGE-ME-strong-password>

# Connection string format
# AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:password@postgres:5432/airflow

# For SQLite (development only)
# AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db

# ============================================================================
# Executor Configuration
# ============================================================================
# Options: SequentialExecutor, LocalExecutor, CeleryExecutor, KubernetesExecutor
AIRFLOW__CORE__EXECUTOR=LocalExecutor  # Use CeleryExecutor or KubernetesExecutor for production

# Celery Configuration (if using CeleryExecutor)
AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:password@postgres:5432/airflow
AIRFLOW__CELERY__WORKER_CONCURRENCY=16
AIRFLOW__CELERY__POOL=prefork

# Kubernetes Configuration (if using KubernetesExecutor)
AIRFLOW__KUBERNETES__NAMESPACE=airflow
AIRFLOW__KUBERNETES__WORKER_CONTAINER_REPOSITORY=apache/airflow
AIRFLOW__KUBERNETES__WORKER_CONTAINER_TAG=2.7.1-python3.11
AIRFLOW__KUBERNETES__KUBE_CLIENT_REQUEST_ARGS={"_request_timeout": 60}

# ============================================================================
# Web Server Configuration
# ============================================================================
AIRFLOW__WEBSERVER__WEB_SERVER_PORT=8082
AIRFLOW__WEBSERVER__BASE_URL=http://localhost:8082
AIRFLOW__WEBSERVER__SECRET_KEY=<CHANGE-ME-generate-with-openssl-rand-base64-32>
AIRFLOW__WEBSERVER__EXPOSE_CONFIG=false  # Disable in production
AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX=true  # Enable if behind reverse proxy

# Authentication
AIRFLOW__WEBSERVER__AUTHENTICATE=true
AIRFLOW__WEBSERVER__AUTH_BACKEND=airflow.api.auth.backend.basic_auth  # basic_auth, kerberos, oauth
AIRFLOW__WEBSERVER__RBAC=true

# Admin User (created on first startup)
AIRFLOW_ADMIN_USERNAME=admin
AIRFLOW_ADMIN_PASSWORD=<CHANGE-ME-admin-password>
AIRFLOW_ADMIN_EMAIL=admin@vericrop.example.com
AIRFLOW_ADMIN_FIRSTNAME=Admin
AIRFLOW_ADMIN_LASTNAME=User

# ============================================================================
# Scheduler Configuration
# ============================================================================
AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT=false  # Don't backfill by default
AIRFLOW__SCHEDULER__MAX_ACTIVE_TASKS_PER_DAG=16
AIRFLOW__SCHEDULER__MAX_ACTIVE_RUNS_PER_DAG=16
AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=300  # seconds
AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL=30  # seconds
AIRFLOW__SCHEDULER__PARSING_PROCESSES=2

# Task Retry Configuration
AIRFLOW__SCHEDULER__DEFAULT_TASK_RETRIES=3
AIRFLOW__SCHEDULER__MIN_RETRY_DELAY=60  # seconds

# ============================================================================
# DAG Configuration
# ============================================================================
AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
AIRFLOW__CORE__LOAD_EXAMPLES=false  # Disable example DAGs in production
AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=true
AIRFLOW__CORE__DAG_DISCOVERY_SAFE_MODE=true
AIRFLOW__CORE__PARALLELISM=32  # Max parallel task instances across all DAGs
AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG=16
AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG=16

# DAG Processing
AIRFLOW__CORE__MIN_SERIALIZED_DAG_UPDATE_INTERVAL=30
AIRFLOW__CORE__MIN_SERIALIZED_DAG_FETCH_INTERVAL=10

# ============================================================================
# Logging Configuration
# ============================================================================
AIRFLOW__LOGGING__BASE_LOG_FOLDER=/opt/airflow/logs
AIRFLOW__LOGGING__REMOTE_LOGGING=false  # Enable for cloud storage
AIRFLOW__LOGGING__LOGGING_LEVEL=INFO
AIRFLOW__LOGGING__FAB_LOGGING_LEVEL=WARN
AIRFLOW__LOGGING__LOG_FORMAT=[%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
AIRFLOW__LOGGING__SIMPLE_LOG_FORMAT=%%(asctime)s %%(levelname)s - %%(message)s
AIRFLOW__LOGGING__COLORED_CONSOLE_LOG=false  # Disable for JSON logging

# Remote Logging (S3, GCS, Azure Blob)
# AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID=aws_default
# AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER=s3://your-bucket/airflow/logs
# AIRFLOW__LOGGING__ENCRYPT_S3_LOGS=true

# ============================================================================
# API Configuration
# ============================================================================
AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth
AIRFLOW__API__ENABLE_EXPERIMENTAL_API=false  # Use stable API
AIRFLOW__API__MAXIMUM_PAGE_LIMIT=100

# ============================================================================
# Security Configuration
# ============================================================================
# Fernet Key (for encrypting connections, variables)
AIRFLOW__CORE__FERNET_KEY=<CHANGE-ME-generate-with-python-cryptography.fernet.Fernet.generate_key()>

# Secrets Backend (optional - for external secrets management)
# AIRFLOW__SECRETS__BACKEND=airflow.providers.hashicorp.secrets.vault.VaultBackend
# AIRFLOW__SECRETS__BACKEND_KWARGS={"connections_path": "connections", "variables_path": "variables", "mount_point": "airflow", "url": "http://vault:8200", "token": "your-token"}

# For AWS Secrets Manager:
# AIRFLOW__SECRETS__BACKEND=airflow.providers.amazon.aws.secrets.secrets_manager.SecretsManagerBackend
# AIRFLOW__SECRETS__BACKEND_KWARGS={"connections_prefix": "airflow/connections", "variables_prefix": "airflow/variables"}

# ============================================================================
# VeriCrop Integration Configuration
# ============================================================================
# Kafka Configuration
KAFKA_BOOTSTRAP_SERVERS=kafka:9092
KAFKA_TOPIC_EVALUATION_REQUESTS=evaluation-requests
KAFKA_TOPIC_EVALUATION_RESULTS=evaluation-results
KAFKA_CONSUMER_GROUP=airflow-consumer-group

# ML Service
ML_SERVICE_URL=http://ml-service:8000
ML_SERVICE_TIMEOUT=60
ML_SERVICE_RETRIES=3

# VeriCrop API
VERICROP_API_URL=http://vericrop-gui:8080/api
VERICROP_API_TIMEOUT=30

# Database
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=vericrop
POSTGRES_USER=vericrop
POSTGRES_PASSWORD=<CHANGE-ME-db-password>

# ============================================================================
# Email Configuration (for alerts)
# ============================================================================
AIRFLOW__SMTP__SMTP_HOST=smtp.gmail.com
AIRFLOW__SMTP__SMTP_STARTTLS=true
AIRFLOW__SMTP__SMTP_SSL=false
AIRFLOW__SMTP__SMTP_PORT=587
AIRFLOW__SMTP__SMTP_MAIL_FROM=airflow@vericrop.example.com
AIRFLOW__SMTP__SMTP_USER=<your-email>
AIRFLOW__SMTP__SMTP_PASSWORD=<your-password>

# ============================================================================
# Performance Tuning
# ============================================================================
# Worker Configuration
AIRFLOW__CELERY__WORKER_AUTOSCALE=16,12  # max,min workers

# Connection Pooling
AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_SIZE=5
AIRFLOW__DATABASE__SQL_ALCHEMY_MAX_OVERFLOW=10
AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_RECYCLE=1800  # seconds

# Task Instance Limits
AIRFLOW__CORE__PARALLELISM=32
AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG=16

# ============================================================================
# Monitoring and Health
# ============================================================================
AIRFLOW__METRICS__STATSD_ON=false
AIRFLOW__METRICS__STATSD_HOST=statsd
AIRFLOW__METRICS__STATSD_PORT=8125
AIRFLOW__METRICS__STATSD_PREFIX=airflow

# Health Check
AIRFLOW__SCHEDULER__HEALTH_CHECK_THRESHOLD=30  # seconds

# ============================================================================
# Development Settings
# ============================================================================
# Enable development features
AIRFLOW__CORE__UNIT_TEST_MODE=false
AIRFLOW__CORE__LOAD_EXAMPLES=false
AIRFLOW__WEBSERVER__RELOAD_ON_PLUGIN_CHANGE=false

# ============================================================================
# Environment
# ============================================================================
ENVIRONMENT=production  # development, staging, production
TZ=UTC  # Timezone
PYTHONUNBUFFERED=1
